from fastapi import FastAPI, File, UploadFile, HTTPException, Query, Body, Depends, Request
from app.kobert_model_loader import predict_scam_kobert 
from app.model_loader import get_model 
from app.utils import process_youtube_video, get_youtube_text, split_text, extract_video_id, apply_keyword_bias
from schemas.script import YouTubeScamResponse, ScriptScamResponse
from torchvision import transforms
from PIL import Image
import torch
import os
import numpy as np
from transformers import BertTokenizer, BertForSequenceClassification
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from sqlalchemy.orm import Session
from .database import engine, Base, get_db
from .models import AnalysisHistory
from fastapi.templating import Jinja2Templates

app = FastAPI(title="ScamGuard AI API")

# í…œí”Œë¦¿ ì„¤ì •
templates = Jinja2Templates(directory="template")
# ì„œë²„ ì‹œì‘ ì‹œ í…Œì´ë¸” ìë™ ìƒì„±
Base.metadata.create_all(bind=engine)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© ì „ì²´ í—ˆìš©
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ë”¥í˜ì´í¬ ëª¨ë¸ ë¡œë“œ ---
model, device = get_model("models/scamguard_model.pth")
# --- KoBERT ì‚¬ê¸° ìë§‰ íƒì§€ ëª¨ë¸ ë¡œë“œ ---
KO_MODEL_PATH = "models/kobert_model"
ko_tokenizer = BertTokenizer.from_pretrained(KO_MODEL_PATH)
ko_model = BertForSequenceClassification.from_pretrained(KO_MODEL_PATH)
ko_model.to(device)
ko_model.eval()

@app.get("/")
def read_root():
    return {"message": "Scam Guard AI Server is Running!"}

# --- ìµœê·¼ ë¶„ì„ ê¸°ë¡ 10ê°œë¥¼ ê°€ì ¸ì˜¤ëŠ” ì—”ë“œí¬ì¸íŠ¸ ---
@app.get("/recent-results")
async def get_recent_results(db: Session = Depends(get_db)):
    return db.query(AnalysisHistory).order_by(AnalysisHistory.created_at.desc()).limit(10).all()

# --- ë”¥í˜ì´í¬ ì „ì²˜ë¦¬ ì„¤ì • ---
transformer = transforms.Compose(
    [
        transforms.ToPILImage(),
        transforms.Resize((299, 299)),
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),
    ]
)

# --- ë”¥í˜ì´í¬ íƒì§€ ì—”ë“œí¬ì¸íŠ¸ ---
@app.post("/deepfake")
async def predict_deepfake_from_url(url: str, db: Session = Depends(get_db)):
    # 1. ìœ íŠœë¸Œì—ì„œ ì–¼êµ´ ì¶”ì¶œ
    face_img = process_youtube_video(url)
    if face_img is None:
        return {
            "url": url,
            "is_fake": False,
            "confidence": 0.0,
            "message": "ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

    # 2. ì „ì²˜ë¦¬ ë° ì¶”ë¡ 
    input_tensor = transformer(face_img).unsqueeze(0).to(device)
    with torch.no_grad():
        output = model(input_tensor)
        prob = torch.nn.functional.softmax(output, dim=1)[0][1].item()

    result = {
        "url": url,
        "is_fake": prob > 0.5,
        "confidence": round(prob * 100, 2),
        "message": "ğŸš¨ ë”¥í˜ì´í¬ ì˜ì‹¬" if prob > 0.5 else "âœ… ì •ìƒ ì˜ìƒ",
    }

    # ë¶„ì„ ì™„ë£Œ ì‹œ DB ìë™ ì €ì¥
    history = AnalysisHistory(
        url=url,
        video_id=extract_video_id(url),
        status=result["message"],
        deepfake_prob=result["confidence"],
        is_fake=1 if result["is_fake"] else 0
    )
    db.add(history)
    db.commit()

    return result


# --- ì‚¬ê¸° íƒì§€ ê³µí†µ ë¶„ì„ ë¡œì§ ìˆ˜í–‰ í•¨ìˆ˜ ---
def run_scam_analysis_logic(chunks):
    scam_results = []
    max_prob = 0.0 
    
    for sentence in chunks:
        # 1. ëª¨ë¸ ê¸°ë°˜ ê¸°ì´ˆ í™•ë¥  ì˜ˆì¸¡
        prob = predict_scam_kobert(sentence)
        # 2. í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ í•©ì‚° ë° ìƒì„¸ ì‚¬ìœ (reason) ì¶”ì¶œ
        prob, detected_info = apply_keyword_bias(sentence, prob)
        
        # ìµœê³  í™•ë¥ ê°’ ê°±ì‹ 
        if prob > max_prob:
            max_prob = prob
        
        # íƒì§€ ì„ê³„ê°’(0.7) ì´ˆê³¼ ì‹œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
        if prob >= 0.7:
            scam_results.append({
                "text": sentence,
                "scam_probability": f"{round(prob * 100, 2)}%",
                "reason": detected_info # íƒì§€ëœ í‚¤ì›Œë“œ ì •ë³´ í¬í•¨
            })

    # ìµœì¢… ìœ„í—˜ ìƒíƒœ íŒë³„ (ìœ„í—˜, ì£¼ì˜, ì•ˆì „)
    if max_prob >= 0.9:
        final_status = "ğŸš¨ ìœ„í—˜"
    elif max_prob >= 0.7:
        final_status = "âš ï¸ ì£¼ì˜"
    else:
        final_status = "âœ… ì•ˆì „"
        
    return max_prob, scam_results, final_status

# --- ìœ íŠœë¸Œ ì˜ìƒ ê¸°ë°˜ ì‚¬ê¸° íƒì§€ ì—”ë“œí¬ì¸íŠ¸ ---
@app.post(
    "/youtube-scam", 
    response_model=YouTubeScamResponse,
    tags=["ìë§‰ ë¶„ì„"], 
    summary="ìœ íŠœë¸Œ ìë§‰ ì‚¬ê¸° íŒë³„"
)
async def analyze_text_scam(
    url: str = Query(..., description="ë¶„ì„í•  ìœ íŠœë¸Œ ì˜ìƒ URL"),
    db: Session = Depends(get_db)
):
    # ë¹„ë””ì˜¤ ID ì¶”ì¶œ ë° ìë§‰ ë°ì´í„° íšë“
    video_id = extract_video_id(url)
    if not video_id:
        raise HTTPException(status_code=400, detail="ID ì¶”ì¶œ ì‹¤íŒ¨")

    raw_text = get_youtube_text(video_id)
    if not raw_text:
        raise HTTPException(status_code=400, detail="ìë§‰ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ëŠ” ì˜ìƒì…ë‹ˆë‹¤.")

    # ìë§‰ ë¶„í•  ë° ê³µí†µ ë¶„ì„ í•¨ìˆ˜ í˜¸ì¶œ
    chunks = split_text(raw_text)
    max_prob, scam_results, final_status = run_scam_analysis_logic(chunks)

    # ë¶„ì„ ì™„ë£Œ ì‹œ DB ìë™ ì €ì¥
    keywords_list = []
    for scam in scam_results:
        for r in scam['reason']:
            keywords_list.append(r['keyword'])

    history = AnalysisHistory(
        url=url,
        video_id=video_id,
        status=final_status,
        scam_prob=round(max_prob * 100, 2),
        keywords=list(set(keywords_list)) # ì¤‘ë³µ ì œê±° í›„ ì €ì¥
    )
    db.add(history)
    db.commit()

    return {
        "url": url,
        "total_sentences": len(chunks),
        "highest_probability": f"{round(max_prob * 100, 2)}%",
        "detected_scams": scam_results,
        "status": final_status
    }

# --- ìë§‰ ê¸°ë°˜ ì‚¬ê¸° íƒì§€ ì—”ë“œí¬ì¸íŠ¸ ---
@app.post(
    "/script-scam", 
    response_model=ScriptScamResponse,
    tags=["ìë§‰ ë¶„ì„"], 
    summary="í…ìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ê¸° íŒë³„"
)
async def analyze_raw_script(
    script: str = Body(..., description="ë¶„ì„í•  ìë§‰ ë˜ëŠ” ëŒ€ë³¸ í…ìŠ¤íŠ¸", embed=True)
):
    if not script or len(script.strip()) < 10:
        raise HTTPException(status_code=400, detail="ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    # í…ìŠ¤íŠ¸ ë¶„í•  ë° ê³µí†µ ë¶„ì„ í•¨ìˆ˜ í˜¸ì¶œ
    chunks = split_text(script)
    max_prob, scam_results, final_status = run_scam_analysis_logic(chunks)

    return {
        "input_summary": script[:50] + "...", 
        "total_sentences": len(chunks),
        "highest_probability": f"{round(max_prob * 100, 2)}%",
        "detected_scams": scam_results,
        "status": final_status
    }

# --- ë°°ì¹˜ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸ ---
@app.get("/test-batch")
async def test_batch_images():
    # 1. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ í´ë” ê²½ë¡œ ì„¤ì •
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    test_dir = os.path.join(base_dir, "test_images")

    if not os.path.exists(test_dir):
        return {"error": "test_images í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    results = []
    # 2. í´ë” ë‚´ íŒŒì¼ë“¤ ë¦¬ìŠ¤íŒ… (png, jpg, jpegë§Œ ê³¨ë¼ë‚´ê¸°)
    image_files = [
        f for f in os.listdir(test_dir) if f.lower().endswith((".png", ".jpg", ".jpeg"))
    ]

    for filename in image_files:
        img_path = os.path.join(test_dir, filename)
        image = Image.open(img_path).convert("RGB")

        # ğŸ’¡ PIL ì´ë¯¸ì§€ë¥¼ Numpy ë°°ì—´ë¡œ ë³€í™˜í•´ì„œ transformerì— ì „ë‹¬
        image_np = np.array(image)
        input_tensor = transformer(image_np).unsqueeze(0).to(device)
        with torch.no_grad():
            output = model(input_tensor)
            prob = torch.nn.functional.softmax(output, dim=1)[0][1].item()

        confidence = round(prob * 100, 2)
        results.append(
            {
                "filename": filename,
                "is_fake": confidence > 50,
                "confidence": f"{confidence}%",
                "status": "ğŸš¨ ë”¥í˜ì´í¬ ì˜ì‹¬" if confidence > 50 else "âœ… ì •ìƒ",
            }
        )

    # 4. ì „ì²´ ê²°ê³¼ ë°˜í™˜
    return {"total_count": len(results), "predictions": results}

@app.get("/web-analysis", response_class=HTMLResponse)
async def get_web_page(request: Request, url: str = Query(None)):
    return templates.TemplateResponse("web_analysis.html", {"request": request, "url": url})