import cv2
import yt_dlp
import os
from pathlib import Path
import re
from youtube_transcript_api import YouTubeTranscriptApi
import pandas as pd

# ì–¼êµ´ íƒì§€ê¸° ì´ˆê¸°í™”
face_cascade = cv2.CascadeClassifier(
    cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
)


def process_youtube_video(url: str):

    ydl_opts = {
        "format": "best[ext=mp4][height<=480]/worst",
        "outtmpl": "temp_video.mp4",
        "quiet": True,
    }
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        ydl.download([url])

    cap = cv2.VideoCapture("temp_video.mp4")
    fps = cap.get(cv2.CAP_PROP_FPS)

    target_frame = int(fps * 6) if fps > 0 else 180
    cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)

    ret, frame = cap.read()
    cap.release()
    if os.path.exists("temp_video.mp4"):
        os.remove("temp_video.mp4")

    if not ret:
        return None

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.1, 4, minSize=(100, 100))

    if len(faces) == 0:
        return None

    (x, y, w, h) = sorted(faces, key=lambda f: f[2] * f[3], reverse=True)[0]

    pad_w = int(w * 0.5)
    pad_h = int(h * 0.5)

    img_h, img_w = frame.shape[:2]
    y1 = max(0, y - pad_h)
    y2 = min(img_h, y + h + pad_h)
    x1 = max(0, x - pad_w)
    x2 = min(img_w, x + w + pad_w)

    face_img = frame[y1:y2, x1:x2]

    face_img_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)
    return face_img_rgb

# ìœ íŠœë¸Œ ì¼ë°˜ ì˜ìƒ ë° ì‡¼ì¸  URLì—ì„œ 11ìë¦¬ IDë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_video_id(url: str):
    regex = r"(?:v=|\/shorts\/|embed\/|youtu.be\/)([a-zA-Z0-9_-]{11})"
    match = re.search(regex, url)
    return match.group(1) if match else None

# ì…ë ¥ê°’ì´ URLì¸ ê²½ìš° IDë§Œ ì¶”ì¶œí•˜ê³ , ì´ë¯¸ IDë¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜
def get_youtube_text(url_or_id):
    video_id = extract_video_id(url_or_id) if "http" in url_or_id else url_or_id
    
    if not video_id:
        print(f"âŒ ìœ íš¨í•œ ë¹„ë””ì˜¤ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url_or_id}")
        return None

    try:
        # 1. API ê°ì²´ ì´ˆê¸°í™”
        ytt_api = YouTubeTranscriptApi()
        
        # 2. list() ë©”ì„œë“œë¡œ í•´ë‹¹ ì˜ìƒì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ìë§‰ ëª©ë¡ì„ ê°€ì ¸ì˜´
        transcript_list = ytt_api.list(video_id)
        
        # 3. í•œêµ­ì–´(ko) > ì˜ì–´(en) ìë§‰ ìˆœìœ¼ë¡œ íƒìƒ‰
        # ìˆ˜ë™ ìë§‰ > ìë™ ìƒì„± ìë§‰ ìˆœìœ¼ë¡œ íƒìƒ‰
        transcript = transcript_list.find_transcript(['ko', 'en'])
        fetched_transcript = transcript.fetch()
        
        # 4. ë°˜í™˜ëœ FetchedTranscript ê°ì²´ë¥¼ ìˆœíšŒí•˜ë©° í…ìŠ¤íŠ¸ë§Œ ê²°í•©
        # ê° snippetì€ .text ì†ì„±ì„ í†µí•´ ìë§‰ ë‚´ìš©ì„ ì œ
        return " ".join([snippet.text for snippet in fetched_transcript])
            
    except Exception as e:
        print(f"âŒ ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨ (ID: {video_id}): {e}")
        return None

# ê¸´ í…ìŠ¤íŠ¸ë¥¼ AI ëª¨ë¸ì´ ì½ê¸° ì¢‹ê²Œ 150ì ë‹¨ìœ„ë¡œ ìª¼ê°œëŠ” í•¨ìˆ˜
def split_text(text, chunk_size=150):
    if not text: return []
    # ê³µë°± ì œê±° ë° ì „ì²˜ë¦¬
    text = re.sub(r'\s+', ' ', str(text)).strip()
    # 150ì ë‹¨ìœ„ë¡œ ë¶„í•  (30ì ë¯¸ë§Œ ë²„ë¦¼)
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size) if len(text[i:i+chunk_size]) > 30]

BASE_DIR = Path(__file__).resolve().parent.parent
KEYWORD_FILE = BASE_DIR / "high_risk_keywords.csv"

# ê³ ìœ„í—˜ í‚¤ì›Œë“œì˜ ê°€ì¤‘ì¹˜ ëˆ„ì í•˜ì—¬ í•©ì‚°
def apply_keyword_bias(text, probability):
    detected_items = []
    try:
        if os.path.exists(KEYWORD_FILE):
            # í‚¤ì›Œë“œ ì„¤ì • íŒŒì¼ ë¡œë“œ
            df = pd.read_csv(KEYWORD_FILE)
            # ê°€ì¤‘ì¹˜ í•©ì‚°ìš© ë³€ìˆ˜ ì´ˆê¸°í™”
            total_boost = 0.0

            for _, row in df.iterrows():
                # ê°œë³„ í‚¤ì›Œë“œ ë° ê°€ì¤‘ì¹˜ ì •ë³´ ì¶”ì¶œ
                word = str(row['keyword']).strip()
                weight = float(row['weight'])
                category = str(row['category'])
                description = str(row['description'])
                
                # ë¬¸ì¥ ë‚´ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ ê²€ì‚¬
                if word in text:
                    # ê°€ì¤‘ì¹˜ ëˆ„ì  í•©ì‚°
                    total_boost += weight
                    # íƒì§€ëœ í‚¤ì›Œë“œ ìƒì„¸ ì •ë³´ ì¶”ê°€
                    detected_items.append({
                        "keyword": word,
                        "category": category,
                        "description": description
                    })
            
            if total_boost > 0:
                # ìµœì¢… í™•ë¥  ê³„ì‚° ë° ìµœëŒ€ì¹˜(0.99) ì œí•œ
                probability = min(0.99, probability + total_boost)
                # í„°ë¯¸ë„ ë¡œê·¸ ì¶œë ¥
                print(f"ğŸš¨ íƒì§€ í‚¤ì›Œë“œ ëª©ë¡: {[item['keyword'] for item in detected_items]}")
    except Exception as e:
        # ì˜ˆì™¸ ë°œìƒ ì‹œ ì˜¤ë¥˜ ë¡œê·¸ ì¶œë ¥
        print(f"âš ï¸ ê°€ì¤‘ì¹˜ ë³´ì • ë¡œì§ ì˜¤ë¥˜: {e}")
            
    # ë³´ì •ëœ í™•ë¥  ë° íƒì§€ ìƒì„¸ ë‚´ì—­ ë°˜í™˜
    return probability, detected_items